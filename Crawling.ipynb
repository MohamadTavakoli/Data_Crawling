{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "# Set up Chrome WebDriver options\n",
    "options = Options()\n",
    "options.add_argument('--blink-settings=imagesEnabled=false')  # Disable loading images\n",
    "# options.add_argument('--enable-print-browser')  # Uncomment if you want to enable printing from the browser (optional)\n",
    "options.add_argument(\"--headless\")  # Run the browser in headless mode (without a GUI)\n",
    "caps = DesiredCapabilities().CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"eager\"  # Set page load strategy to eager\n",
    "\n",
    "# Import additional libraries and modules\n",
    "import time\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Set a custom User-Agent header for web requests\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "import urllib.request\n",
    "\n",
    "# Read a list of tickers from an Excel file and sort them\n",
    "tickers = list(pd.read_excel('لیست صندوق_های سهامی.xlsx')['نماد'].values)\n",
    "tickers.sort()\n",
    "\n",
    "# Define a function to extract the date and check for \"اصلاحیه\" (correction) in a text item\n",
    "def date_gen(item):\n",
    "    text = item.text\n",
    "    loc = text.find('/')\n",
    "    date = int(text[loc - 4 : loc + 6].replace('/', ''))\n",
    "    eslahie = 'اصلاحیه' in text\n",
    "    return date, eslahie\n",
    "\n",
    "# Define a function to check if two dates match\n",
    "def date_check(date, sent):\n",
    "    date_m = (((date // 100) % 100) + 1) % 12\n",
    "    sent_m = ((sent // 100) % 100) % 12\n",
    "    date_y = date // 10000 if date_m != 1 else (date // 10000) + 1\n",
    "    sent_y = sent // 10000\n",
    "    return (date_m == sent_m) and (date_y == sent_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting URLs and web scraping configuration\n",
    "link_start = 'https://codal.ir'  # Base URL\n",
    "start_url = \"https://codal.ir/ReportList.aspx?search&Symbol=\"  # URL to start scraping from\n",
    "end_url = \"&LetterType=-1&FromDate=1398%2F01%2F01&Isic=46430170&AuditorRef=-1&Audited&NotAudited&IsNotAudited=false&Childs&Mains&Publisher=false&CompanyState=2&Category=3&CompanyType=-1&Consolidatable&NotConsolidatable&PageNumber=\"\n",
    "driver = webdriver.Chrome(options=options, desired_capabilities=caps)  # Initialize a headless Chrome WebDriver\n",
    "\n",
    "# Iterate over a list of tickers\n",
    "for ticker in tickers:\n",
    "    # Create a directory for each ticker\n",
    "    os.mkdir(os.getcwd() + '/New_Data/' + ticker)\n",
    "    \n",
    "    page = '1'  # Start on page 1\n",
    "    url = start_url + ticker + end_url + page  # Construct the URL for the first page of reports for a ticker\n",
    "    driver.get(url)  # Open the URL in the browser\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'letter-title')))  # Wait for the page to load\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')  # Parse the page source with BeautifulSoup\n",
    "    \n",
    "    try:\n",
    "        # Extract the number of pages from the last page link\n",
    "        page_num = int(soup.find_all('li', {'title': 'آخرین صفحه'})[0].find('a')['href'].split('=')[-1])\n",
    "    except:\n",
    "        page_num = 1  # If no page number is found, assume there's only one page\n",
    "    \n",
    "    a = []  # Initialize a list for report titles\n",
    "    sent = []  # Initialize a list for report sending times\n",
    "    \n",
    "    # Extract report titles and sending times from the first page\n",
    "    a += soup.find_all('a', class_='letter-title')\n",
    "    sent += soup.find_all('td', {'data-heading': 'زمان ارسال'})\n",
    "    \n",
    "    if page_num != 1:\n",
    "        # If there are more than one page, loop through the remaining pages\n",
    "        for page in range(2, page_num + 1):\n",
    "            url = start_url + ticker + end_url + str(page)  # Construct the URL for the next page\n",
    "            driver.get(url)  # Open the URL in the browser\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'letter-title')))  # Wait for the page to load\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')  # Parse the page source with BeautifulSoup\n",
    "            a += soup.find_all('a', class_='letter-title')  # Extract report titles\n",
    "            sent += soup.find_all('td', {'data-heading': 'زمان ارسال'})  # Extract report sending times\n",
    "\n",
    "    # Extract dates and \"اصلاحیه\" flags from the report titles\n",
    "    dates = [date_gen(x)[0] for x in a]\n",
    "    eslahie = [date_gen(x)[1] for x in a]\n",
    "\n",
    "    # Extract sending dates from the sending time cells\n",
    "    sent_dates = [date_gen(td.findChild())[0] for td in sent]\n",
    "\n",
    "    # Extract report links and build complete URLs\n",
    "    links = [link_start + x['href'] for x in a]\n",
    "\n",
    "    j = 0  # Counter for handling multiple files with the same date\n",
    "    for i, link in enumerate(links):\n",
    "        r = requests.get(link, headers=headers, allow_redirects=False)  # Send a GET request to the report page\n",
    "        soup2 = BeautifulSoup(r.text, 'html.parser')  # Parse the report page\n",
    "\n",
    "        imgs = soup2.find_all('img')  # Find all image elements on the report page\n",
    "        for img in imgs:\n",
    "            if 'xls' in img['src']:\n",
    "                # Extract the download link for the Excel file\n",
    "                download_link = 'https://codal.ir/Reports/' + img.parent.parent['onclick'].split(\"'\")[1]\n",
    "                excel_file = requests.get(download_link, headers=headers, allow_redirects=True)  # Download the Excel file\n",
    "\n",
    "                if (date_check(dates[i], sent_dates[i])) and not (eslahie[i]):\n",
    "                    # Check if the date matches the sending date and if it's not a correction report\n",
    "                    open(os.getcwd() + '/New_Data/' + ticker + '/' + str(dates[i]) + '.xlsx', 'wb').write(excel_file.content)\n",
    "                else:\n",
    "                    # If it's a correction report or the date doesn't match, add a suffix to the file name\n",
    "                    open(os.getcwd() + '/New_Data/' + ticker + '/' + str(dates[i]) + '_check' + str(j) + '.xlsx', 'wb').write(excel_file.content)\n",
    "                    j += 1\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    path = os.getcwd() + '/New_Data/' + ticker + '/'\n",
    "    naames = os.listdir(path)\n",
    "    for name in naames:\n",
    "        if 'check' in name:\n",
    "            print(ticker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
